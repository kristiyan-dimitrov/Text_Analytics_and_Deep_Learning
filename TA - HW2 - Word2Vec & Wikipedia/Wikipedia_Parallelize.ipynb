{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Kristiyan Dimitrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim import utils\n",
    "import gensim, logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import gzip\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26603591 enwiki-latest.json.gz\n"
     ]
    }
   ],
   "source": [
    "# This shows the number of lines\n",
    "!wc -l enwiki-latest.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sentence:str):\n",
    "    \n",
    "    return [word for word in tokenized_sentence if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def preprocess_text(text:str):\n",
    "    \"\"\"Converts text to sentences; tokenizes sentences; removes all stopword tokens\"\"\"\n",
    "        \n",
    "    # Split text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Preprocess each sentence converting it into a list of lower case tokens, which are not too long or too short\n",
    "    tokenized_sentences = [gensim.utils.simple_preprocess(sent) for sent in sentences]\n",
    "    \n",
    "    # Remove any tokens which are stop words while preserving the sentence structure of the article (i.e. list of lists)\n",
    "    no_stops = [remove_stopwords(tokenized_sentence) for tokenized_sentence in tokenized_sentences]\n",
    "        \n",
    "    return no_stops\n",
    "\n",
    "\n",
    "def process_article(article):\n",
    "    \"\"\"Preprocesses all the sections of a Wikipedia article, then\n",
    "       save to txt file\"\"\"\n",
    "    \n",
    "    if article['title']+'.txt' in os.listdir('data'):\n",
    "#         print(f\"{article['title']} already processed\")\n",
    "        pass\n",
    "        \n",
    "    article_sentences = list()\n",
    "    \n",
    "    for text in article['section_texts']:\n",
    "        article_sentences.extend(preprocess_text(text))\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join('data', article['title']+'.txt'), 'w') as filehandle:\n",
    "            for listitem in article_sentences:\n",
    "                filehandle.write(f'{\" \".join(listitem)}\\n')\n",
    "    except Exception as e:\n",
    "            print(f\"FAILED TO SAVE {article['title']}\")\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "def take_batch(filehandle, batch_size:int):\n",
    "    \n",
    "    batch_of_lines = list()\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        \n",
    "        batch_of_lines.append(json.loads(filehandle.readline()))\n",
    "    \n",
    "    return batch_of_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batch_size, n_batches, n_cpus):\n",
    "\n",
    "    with gzip.open('enwiki-latest.json.gz', 'r') as f:\n",
    "\n",
    "        counter=0\n",
    "    \n",
    "        while counter < batch_size * n_batches:\n",
    "\n",
    "            batch_of_articles = take_batch(f, batch_size)\n",
    "\n",
    "            pool=multiprocessing.Pool(n_cpus) # multiprocessing.cpu_count()\n",
    "\n",
    "            pool.map(process_article, batch_of_articles)\n",
    "\n",
    "            pool.close()\n",
    "            \n",
    "#             if counter % 1000 == 0:\n",
    "            print(counter)\n",
    "\n",
    "            counter += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO SAVE Dragon 32/64\n",
      "[Errno 2] No such file or directory: 'data/Dragon 32/64.txt'\n",
      "FAILED TO SAVE Alliance 90/The Greens\n",
      "[Errno 2] No such file or directory: 'data/Alliance 90/The Greens.txt'\n",
      "FAILED TO SAVE ISO/IEC 8859-1\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 8859-1.txt'\n",
      "FAILED TO SAVE ISO/IEC 8859\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 8859.txt'\n",
      "FAILED TO SAVE IC 342/Maffei Group\n",
      "[Errno 2] No such file or directory: 'data/IC 342/Maffei Group.txt'\n",
      "FAILED TO SAVE Aoraki / Mount Cook\n",
      "[Errno 2] No such file or directory: 'data/Aoraki / Mount Cook.txt'\n",
      "0\n",
      "FAILED TO SAVE PL/I\n",
      "[Errno 2] No such file or directory: 'data/PL/I.txt'\n",
      "FAILED TO SAVE OS/2\n",
      "[Errno 2] No such file or directory: 'data/OS/2.txt'\n",
      "FAILED TO SAVE IBM System/360\n",
      "[Errno 2] No such file or directory: 'data/IBM System/360.txt'\n",
      "FAILED TO SAVE 56 kbit/s line\n",
      "[Errno 2] No such file or directory: 'data/56 kbit/s line.txt'\n",
      "FAILED TO SAVE Z/OS\n",
      "[Errno 2] No such file or directory: 'data/Z/OS.txt'\n",
      "FAILED TO SAVE OS/390\n",
      "[Errno 2] No such file or directory: 'data/OS/390.txt'\n",
      "10000\n",
      "FAILED TO SAVE TAT-12/13\n",
      "[Errno 2] No such file or directory: 'data/TAT-12/13.txt'\n",
      "FAILED TO SAVE McDonnell Douglas F/A-18 Hornet\n",
      "[Errno 2] No such file or directory: 'data/McDonnell Douglas F/A-18 Hornet.txt'\n",
      "FAILED TO SAVE ISO/IEC 8859-15\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 8859-15.txt'\n",
      "FAILED TO SAVE CP/M\n",
      "[Errno 2] No such file or directory: 'data/CP/M.txt'\n",
      "FAILED TO SAVE Baltimore/Washington International Airport\n",
      "[Errno 2] No such file or directory: 'data/Baltimore/Washington International Airport.txt'\n",
      "FAILED TO SAVE ISO/IEC 7816\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 7816.txt'\n",
      "FAILED TO SAVE IBM System/370\n",
      "[Errno 2] No such file or directory: 'data/IBM System/370.txt'\n",
      "20000\n",
      "FAILED TO SAVE ISO/IEC 7811\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 7811.txt'\n",
      "FAILED TO SAVE Riot/Clone\n",
      "[Errno 2] No such file or directory: 'data/Riot/Clone.txt'\n",
      "FAILED TO SAVE John Lennon/Plastic Ono Band\n",
      "[Errno 2] No such file or directory: 'data/John Lennon/Plastic Ono Band.txt'\n",
      "FAILED TO SAVE Discredited HIV/AIDS origins theories\n",
      "[Errno 2] No such file or directory: 'data/Discredited HIV/AIDS origins theories.txt'\n",
      "FAILED TO SAVE HIV/AIDS in the United States\n",
      "[Errno 2] No such file or directory: 'data/HIV/AIDS in the United States.txt'\n",
      "FAILED TO SAVE MP/M\n",
      "[Errno 2] No such file or directory: 'data/MP/M.txt'\n",
      "FAILED TO SAVE List of VFL/AFL premiers\n",
      "[Errno 2] No such file or directory: 'data/List of VFL/AFL premiers.txt'\n",
      "30000\n",
      "FAILED TO SAVE 2/1 game forcing\n",
      "[Errno 2] No such file or directory: 'data/2/1 game forcing.txt'\n",
      "FAILED TO SAVE Judbarra / Gregory National Park\n",
      "[Errno 2] No such file or directory: 'data/Judbarra / Gregory National Park.txt'\n",
      "FAILED TO SAVE Joint United Nations Programme on HIV/AIDS\n",
      "[Errno 2] No such file or directory: 'data/Joint United Nations Programme on HIV/AIDS.txt'\n",
      "40000\n",
      "50000\n",
      "FAILED TO SAVE ST506/ST412\n",
      "[Errno 2] No such file or directory: 'data/ST506/ST412.txt'\n",
      "60000\n",
      "FAILED TO SAVE BOS/360\n",
      "[Errno 2] No such file or directory: 'data/BOS/360.txt'\n",
      "FAILED TO SAVE Karlu Karlu / Devils Marbles Conservation Reserve\n",
      "[Errno 2] No such file or directory: 'data/Karlu Karlu / Devils Marbles Conservation Reserve.txt'\n",
      "70000\n",
      "FAILED TO SAVE S/PDIF\n",
      "[Errno 2] No such file or directory: 'data/S/PDIF.txt'\n",
      "FAILED TO SAVE AC/DC\n",
      "[Errno 2] No such file or directory: 'data/AC/DC.txt'\n",
      "FAILED TO SAVE Upholder/Victoria-class submarine\n",
      "[Errno 2] No such file or directory: 'data/Upholder/Victoria-class submarine.txt'\n",
      "FAILED TO SAVE AN/PRC-77 Portable Transceiver\n",
      "[Errno 2] No such file or directory: 'data/AN/PRC-77 Portable Transceiver.txt'\n",
      "FAILED TO SAVE FK Bodø/Glimt\n",
      "[Errno 2] No such file or directory: 'data/FK Bodø/Glimt.txt'\n",
      "FAILED TO SAVE IBM Series/1\n",
      "[Errno 2] No such file or directory: 'data/IBM Series/1.txt'\n",
      "FAILED TO SAVE CP/M-86\n",
      "[Errno 2] No such file or directory: 'data/CP/M-86.txt'\n",
      "80000\n",
      "FAILED TO SAVE UN/CEFACT\n",
      "[Errno 2] No such file or directory: 'data/UN/CEFACT.txt'\n",
      "FAILED TO SAVE 9/11 (2002 film)\n",
      "[Errno 2] No such file or directory: 'data/9/11 (2002 film).txt'\n",
      "FAILED TO SAVE Apollo/Domain\n",
      "[Errno 2] No such file or directory: 'data/Apollo/Domain.txt'\n",
      "FAILED TO SAVE Radio Free Europe/Radio Liberty\n",
      "[Errno 2] No such file or directory: 'data/Radio Free Europe/Radio Liberty.txt'\n",
      "FAILED TO SAVE Na+/K+-ATPase\n",
      "[Errno 2] No such file or directory: 'data/Na+/K+-ATPase.txt'\n",
      "FAILED TO SAVE 81P/Wild\n",
      "[Errno 2] No such file or directory: 'data/81P/Wild.txt'\n",
      "FAILED TO SAVE And/or\n",
      "[Errno 2] No such file or directory: 'data/And/or.txt'\n",
      "FAILED TO SAVE ISO/IEC 7810\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 7810.txt'\n",
      "FAILED TO SAVE ISO/IEC 14443\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 14443.txt'\n",
      "FAILED TO SAVE AN/USQ-17\n",
      "[Errno 2] No such file or directory: 'data/AN/USQ-17.txt'\n",
      "FAILED TO SAVE ISO/IEC 646\n",
      "[Errno 2] No such file or directory: 'data/ISO/IEC 646.txt'\n",
      "FAILED TO SAVE GBU-43/B MOAB\n",
      "[Errno 2] No such file or directory: 'data/GBU-43/B MOAB.txt'\n",
      "FAILED TO SAVE T-54/T-55\n",
      "[Errno 2] No such file or directory: 'data/T-54/T-55.txt'\n",
      "FAILED TO SAVE AN/USQ-20\n",
      "[Errno 2] No such file or directory: 'data/AN/USQ-20.txt'\n",
      "FAILED TO SAVE MUSIC/SP\n",
      "[Errno 2] No such file or directory: 'data/MUSIC/SP.txt'\n",
      "FAILED TO SAVE AN/UYK-8\n",
      "[Errno 2] No such file or directory: 'data/AN/UYK-8.txt'\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "# Start at 7:22 PM\n",
    "run(batch_size = 10_000, n_batches = 10, n_cpus = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 16s ± 5.38 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# 12 cpus working on 48 articles at a time; 10 batches\n",
    "%timeit run(batch_size=48,n_batches=10,n_cpus=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 10s ± 5.02 s per loop (mean ± std. dev. of 7 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "# 6 cpus working on 48 articles at a time; 10 batches\n",
    "%timeit -n2 run(batch_size=48,n_batches=10,n_cpus=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# 10 cpus working on 100 articles at a time; 10 batches\n",
    "%timeit -r1 run(batch_size=100,n_batches=10,n_cpus=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 52s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# 10 cpus working on 1000 articles at a time; 1 batches\n",
    "%timeit -r1 run(batch_size=1000,n_batches=1,n_cpus=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 12 cpus working on 48 articles at a time; 10 batches -> \n",
    "1min 16s ± 5.38 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "- 6 cpus working on 48 articles at a time; 10 batches\n",
    "1min 10s ± 5.02 s per loop (mean ± std. dev. of 7 runs, 2 loops each)\n",
    "- 10 cpus working on 100 articles at a time; 10 batches\n",
    "2min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
    "- 10 cpus working on 1000 articles at a time; 1 batches\n",
    "1min 52s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
    "\n",
    "From the above, I suspect that larger batches work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            if fname in ['.ipynb_checkpoints', '.DS_Store']:\n",
    "                continue\n",
    "#             print(fname)\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "#                 yield bytes(line, 'utf-8').decode('utf-8', 'ignore').split()\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model parameters are:\n",
    "- size = 100\n",
    "- window = 5\n",
    "- sg default=0 (short for skip-gram){0,1} 1 for skip-gram, 0 for CBOW\n",
    "- hs default=0 (short for hierarchical softmax) if 1 then negative sampling is used\n",
    "- min_count=5 ignores all words with total frequency below this\n",
    "- workers = 3\n",
    "- iter = 5 (number of runs over data; the first run is to built the vocabulary; the remaining ones are training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Below you will see A LOT of logging messages; scroll down to the end for some qualitative model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = MySentences('data') # a memory-friendly iterator\n",
    "\n",
    "# model = gensim.models.Word2Vec(sentences)\n",
    "model = gensim.models.Word2Vec(sentences, min_count=25, size=300, workers=10, sg=0, iter=5, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12min 8s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%timeit -r1 gensim.models.Word2Vec(sentences, min_count=25, size=300, workers=10, sg=0, iter=5, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can potentially save the model and keep training it later?\n",
    "model.save(\"word2vec_100k.model\")\n",
    "# model = gensim.models.Word2Vec.load(\"word2vec_default.model\")\n",
    "model = gensim.models.Word2Vec.load(\"word2vec_100k.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prostitute', 0.508292555809021)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','queen'], negative=['king'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 0.6379384994506836)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['green','red'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sphynx', 0.4912039637565613)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat','cats'], negative=['dog'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('men', 0.4589303731918335)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['male','man'], negative=['female'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estuary', 0.6394079327583313)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['continent', 'river'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('four', 0.9416286945343018)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['two', 'three'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('employment', 0.6662395596504211)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['jobs'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122749"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122749, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

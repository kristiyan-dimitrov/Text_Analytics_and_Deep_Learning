{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from urllib import request\n",
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "# Stemming / Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176967"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sourcing this text document which consists of the book Crime and Punishment\n",
    "# This is part of the nltk book instructions found here: https://www.nltk.org/book/ch03.html\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.4 ms ± 415 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# with regex\n",
    "re_tokens = re.findall('\\w+', raw)\n",
    "\n",
    "%timeit re.findall('\\w+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words from regex:  212001\n",
      "Size of vocabulary:  10729\n"
     ]
    }
   ],
   "source": [
    "print('Number of words from regex: ', len(re_tokens))\n",
    "print('Size of vocabulary: ', len(set(re_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 s ± 17.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# with nltk\n",
    "nltk_tokens = word_tokenize(raw)\n",
    "\n",
    "%timeit word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words from regex:  257727\n",
      "Size of vocabulary:  11539\n"
     ]
    }
   ],
   "source": [
    "print('Number of words from nltk: ', len(nltk_tokens))\n",
    "print('Size of vocabulary: ', len(set(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['passers', '64', 'menage_', 'india', 'Tailor', 'tête', 'merry', 'tilted', 'approval', 'F', 'abasement', 'sparrow', 'turvy', 'cannot', 'hearted', '6', 'self', 'reliance', '6221541', '7', 'Strong', 'sill', 'knacks', 'Literally', 'topsy', 'serfdom', 'crest', 'Holstein', 'E', 'Cannot', 'maidish', 'cheeked', 'sized', 'skinned', 'UTF', 'ton', 'comer', 'tray', 'Folk', 'flint', 'chop', 'riff', '_Pani_', 'passer', 'ant', 'bye', 'bred', 'raff', 'intentioned', 'freshly']\n"
     ]
    }
   ],
   "source": [
    "# A sample of tokens captured by re, but not by nltk\n",
    "print(list(set(re_tokens)-set(nltk_tokens))[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reason.', 'solid-looking', 'late.', 'off.', 'common-sense', 'minute.', 'Tut-tut-tut', 'deal.', 'well-to-do', 'unfortunate.', 'night.', 'Cough-cough', 'alone.', 'dressed.', 'half-way', '\\ufeffThe', 'ill-treated', 'well.', 'began.', 'half-educated', 'egg-shells', 'detail.', 'five-and-thirty', 'funeral.', 'attention.', 'Svidrigaïlov.', 'king.', 'poor-looking', 'cart-horses', 'bell-ringing', 'truth-like', 'free-thinking', 'healthy-looking', 'old-maidish', 'listen.', 'then.', 'minutes.', 'pot-house', 'promise.', 'initial.', 'grown-up', 'yes.', 'hours.', 'begin.', 'pavement.', 'He-he', 'colours.', 'quietly.', 'eating-houses', '[']\n"
     ]
    }
   ],
   "source": [
    "# A sample of tokens captured by nltk, but not by re\n",
    "print(list(set(nltk_tokens)-set(re_tokens))[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SpaCy\n",
    "# Initializing English language\n",
    "nlp = English()\n",
    "\n",
    "# blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 ms ± 46.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "spacy_tokens = tokenizer(raw)\n",
    "\n",
    "%timeit tokenizer(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words from spacy:  224902\n"
     ]
    }
   ],
   "source": [
    "print('Number of words from spacy: ', len(spacy_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "Regex are much faster, but catch way fewer words, which is probably a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
    "len(set(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663 ms ± 5.78 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [wordnet_lemmatizer.lemmatize(token) for token in nltk_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # small english model\n",
    "\n",
    "'''\n",
    "When I tried to directly use the English model on the raw text \n",
    "I get an error that the text is larger than 1,000,000, which is the limit.\n",
    "I am warned that I need ~1GB of temporary memory per 100,000 characters of text, if using the entire pipeline\n",
    "The model has 3 steps in its pipeline, which execute after the tokenizer:\n",
    "tokenizer --> tagger --> parser --> ner\n",
    "So, I will disable the parser and ner, leaving just the (POS) tagger\n",
    "'''\n",
    "\n",
    "nlp.disable_pipes('parser', 'ner')\n",
    "nlp.max_length = 1_500_000 # increasing maximum number of characters\n",
    "doc = nlp(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.78 s ± 143 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nlp(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I originally tried just the tokenizer (without the tagger) and the average was ~109ms. \n",
    "In other words, including the tagger added ~5.5 seconds to the runtime. A significant slowdown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274765"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  dinners\n",
      "Lemma:  dinner\n",
      "POS:  NOUN\n",
      "Tag:  NNS\n",
      "Shape:  xxxx\n",
      "Dependency:    # This is None, because we disabled the parser!\n"
     ]
    }
   ],
   "source": [
    "# Let's look at some of the interesting things that we got for a random word\n",
    "print('Text: ', doc[1204].text)\n",
    "print('Lemma: ', doc[1204].lemma_)\n",
    "print('POS: ', doc[1204].pos_)\n",
    "print('Tag: ', doc[1204].tag_)\n",
    "print('Shape: ', doc[1204].shape_)\n",
    "print('Dependency: ', doc[1204].dep_, ' # This is None, because we disabled the parser!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, plural'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's pretty cool how you can ask SpaCy to explain an acronym!\n",
    "spacy.explain('NNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Regular Expression for all emails in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''This is some sample text with some @email addresses.\n",
    "Note that in the previous sentence there is the special symbol \"@\", which is typical of emails.\n",
    "However, there is no word in front of it. It would also be problematic if we have @ without anything after it either.\n",
    "My regex should capture my own email: kristiyan.t.dimitrov@gmail.com. I need to be careful to capture all the full-stop-separated words;\n",
    "as well as the full-stop at the end.\n",
    "Here is another email@example.com, just for testing purposes.\n",
    "Also, what about kristiyan_dimitrov@mail.com'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kristiyan.t.dimitrov@gmail.com',\n",
       " 'email@example.com',\n",
       " 'kristiyan_dimitrov@mail.com']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'[a-zA-Z0-9_\\.\\#\\!\\?]+@\\w+\\.\\w+'\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 - Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04/12/2019']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = '''This is some text with dates such as 04/12/2019 or another date being April 20th 2019 or December 23th 2020, or even what we do in Bulgaria which is 23.12.1991'''\n",
    "\n",
    "pattern1 = r'\\d{1,2}/\\d{1,2}/\\d{4}'\n",
    "re.findall(pattern1, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['April 20th 2019', 'December 23th 2020']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern2 = r'\\w+ \\d{2}\\w{2} \\d{4}'\n",
    "re.findall(pattern2, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23.12.1991']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern3 = r'\\d{2}\\.\\d{2}\\.\\d{4}'\n",
    "re.findall(pattern3, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kristiyan/Documents/MSiA 490 - Text Analytics/kristiyan-dimitrov_msia_text_analytics_2020'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 1 -----\n",
      "dummy_folder\n",
      "['dummy_subfolder_2', 'dummy_subfolder_1', '.ipynb_checkpoints']\n",
      "['file_in_main_folder.txt']\n",
      "----- 2 -----\n",
      "dummy_folder/dummy_subfolder_2\n",
      "['.ipynb_checkpoints']\n",
      "['file_subfolder_2.txt']\n",
      "----- 3 -----\n",
      "This folder is in .ipynb_checkpoints, so skipping\n",
      "----- 3 -----\n",
      "dummy_folder/dummy_subfolder_1\n",
      "['.ipynb_checkpoints']\n",
      "['file_subfolder_1.txt']\n",
      "----- 4 -----\n",
      "This folder is in .ipynb_checkpoints, so skipping\n",
      "----- 4 -----\n",
      "This folder is in .ipynb_checkpoints, so skipping\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "i = 1\n",
    "for root, subdirs, files in os.walk('dummy_folder'):\n",
    "#     while True:\n",
    "    print('-----', i, '-----')\n",
    "    if '.ipynb_checkpoints' in root:\n",
    "        print('This folder is in .ipynb_checkpoints, so skipping')\n",
    "        continue\n",
    "    print(root)\n",
    "    print(subdirs)\n",
    "    print(files)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20_newsgroups/talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "\n",
    "walk_dir = '20_newsgroups'\n",
    "\n",
    "for root, subdirs, files in os.walk(walk_dir):\n",
    "    list_file_path = os.path.join(root, 'emails.txt')\n",
    "\n",
    "    with open(list_file_path, 'w') as list_file:\n",
    "\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            with io.open(file_path, 'r', encoding='windows-1252') as f:\n",
    "                f_content = f.read()\n",
    "                pattern = r'[a-zA-Z0-9_\\.\\#\\!\\?]+@\\w+\\.\\w+'\n",
    "                emails = re.findall(pattern, f_content)\n",
    "                for email in emails:\n",
    "                    list_file.write(email)\n",
    "                    list_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANUP SCRIPT\n",
    "walk_dir = '20_newsgroups'\n",
    "\n",
    "for root, subdirs, files in os.walk(walk_dir):\n",
    "    list_file_path = os.path.join(root, 'emails.txt')\n",
    "    if '.ipynb_checkpoints' in list_file_path:\n",
    "        continue\n",
    "    try:\n",
    "        os.remove(list_file_path)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

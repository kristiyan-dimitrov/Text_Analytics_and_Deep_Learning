{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be working with the Yelp dataset of reviews: https://www.yelp.com/dataset.\n",
    "More specifically, I will be using the review dataset, which contains integer ratings (1-5) and accompanying text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords\n",
    "# from p_tqdm import p_map\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 kristiyan  staff  6325565224 Dec 13  2019 yelp_data/yelp_academic_dataset_review.json\n"
     ]
    }
   ],
   "source": [
    "# Size of dataset\n",
    "!ls -l yelp_data/yelp_academic_dataset_review.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 6.33 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8021122 yelp_data/yelp_academic_dataset_review.json\n"
     ]
    }
   ],
   "source": [
    "# Number of reviews\n",
    "!wc -l yelp_data/yelp_academic_dataset_review.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 8 million reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing .5 million reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.getcwd(),'yelp_data', 'yelp_academic_dataset_review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_yelp_reviews(datapath, n_reviews=500_000):\n",
    "    \n",
    "    # Initialize empty output\n",
    "    reviews = []\n",
    "    ratings = []\n",
    "    counter = 0\n",
    "    \n",
    "    with open(datapath, 'r', encoding='utf8') as filehandle:\n",
    "        \n",
    "        while counter < n_reviews:\n",
    "            \n",
    "            line = json.loads(filehandle.readline())\n",
    "            \n",
    "            if counter % 99_000 == 0:\n",
    "                print(f'Accessed {counter} reviews')\n",
    "                \n",
    "            reviews.append(line['text'])\n",
    "            ratings.append(line['stars'])\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "    return reviews, ratings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessed 0 reviews\n",
      "Accessed 100000 reviews\n",
      "Accessed 200000 reviews\n",
      "Accessed 300000 reviews\n",
      "Accessed 400000 reviews\n"
     ]
    }
   ],
   "source": [
    "reviews, ratings = access_yelp_reviews(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdffcb1a0d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(ratings).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    0.440750\n",
       "4.0    0.225604\n",
       "1.0    0.140936\n",
       "3.0    0.111556\n",
       "2.0    0.081154\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ratings).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_sentence:str):\n",
    "    \n",
    "    return [word for word in tokenized_sentence if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def preprocess_text(text:str):\n",
    "    \"\"\"Converts text to sentences; tokenizes sentences; removes all stopword tokens\"\"\"\n",
    "    \n",
    "    # Preprocess each sentence converting it into a list of lower case tokens, which are not too long or too short\n",
    "    tokenized_text = gensim.utils.simple_preprocess(text)\n",
    "    \n",
    "    # Remove any tokens which are stop words\n",
    "    no_stops = remove_stopwords(tokenized_text)\n",
    "        \n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool=multiprocessing.Pool(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 pool.map(preprocess_text, reviews[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm expecting this to take ~ half an hour; Time to go to the supermarket!\n",
    "preprocessed_reviews = pool.map(preprocess_text, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving preprocessed reviews to pickle file\n",
    "# with open('preprocessed_reviews.pkl', 'wb') as f:\n",
    "#     pickle.dump(preprocessed_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the pickled processed reviews\n",
    "with open('preprocessed_reviews.pkl', 'rb') as f:\n",
    "    preprocessed_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words = 0\n",
    "number_characters = 0\n",
    "for review in preprocessed_reviews:\n",
    "    for word in review:\n",
    "        number_words += 1\n",
    "        number_characters += len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26637338"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150093818"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.634715375838232"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_characters/number_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 26.6 million words with a total of 150 million characters, giving us an average word length of 5.634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to BOW and TFIDF with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the preprocessed_reviews as entire sentences, not lists of tokens\n",
    "preprocessed_reviews_strings = [\" \".join(review) for review in preprocessed_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving preprocessed reviews with bigrams to pickle file\n",
    "# with open('preprocessed_reviews_strings.pkl', 'wb') as f:\n",
    "#     pickle.dump(preprocessed_reviews_strings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the pickled preprocessed reviews with bigrams\n",
    "with open('preprocessed_reviews_strings.pkl', 'rb') as f:\n",
    "    preprocessed_reviews_strings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x34936 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 28756260 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(lowercase=False, ngram_range =(1,2), max_df = .5, min_df = 100)\n",
    "uni_and_bigram_bow_corpus = count_vectorizer.fit_transform(preprocessed_reviews_strings)\n",
    "uni_and_bigram_bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x11147 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21615399 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW unigrams - Also same value as from gensim Dictionary and filter extremes!\n",
    "count_vectorizer = CountVectorizer(lowercase=False, ngram_range =(1,1), max_df = .5, min_df = 100)\n",
    "unigram_bow_corpus = count_vectorizer.fit_transform(preprocessed_reviews_strings)\n",
    "unigram_bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x11147 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21615399 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF unigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False, ngram_range =(1,1), max_df = .5, min_df = 100)\n",
    "unigram_tfidf_corpus = tfidf_vectorizer.fit_transform(preprocessed_reviews_strings)\n",
    "unigram_tfidf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x34936 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28756260 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF unigrams and bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False, ngram_range =(1,2), max_df = .5, min_df = 100)\n",
    "uni_and_bigram_tfidf_corpus = tfidf_vectorizer.fit_transform(preprocessed_reviews_strings)\n",
    "uni_and_bigram_tfidf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving preprocessed reviews with bigrams to pickle file\n",
    "with open('uni_and_bigram_bow_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(uni_and_bigram_bow_corpus, f)\n",
    "\n",
    "# Saving preprocessed reviews with bigrams to pickle file\n",
    "with open('unigram_bow_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(unigram_bow_corpus, f)\n",
    "\n",
    "# Saving preprocessed reviews with bigrams to pickle file\n",
    "with open('unigram_tfidf_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(unigram_tfidf_corpus, f)\n",
    "\n",
    "# Saving preprocessed reviews with bigrams to pickle file\n",
    "with open('uni_and_bigram_tfidf_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(uni_and_bigram_tfidf_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in saved corpuses\n",
    "with open('uni_and_bigram_bow_corpus.pkl', 'rb') as f:\n",
    "    uni_and_bigram_bow_corpus=pickle.load(f)\n",
    "\n",
    "\n",
    "with open('unigram_bow_corpus.pkl', 'rb') as f:\n",
    "    unigram_bow_corpus=pickle.load(f)\n",
    "\n",
    "\n",
    "with open('unigram_tfidf_corpus.pkl', 'rb') as f:\n",
    "    unigram_tfidf_corpus=pickle.load(f)\n",
    "\n",
    "\n",
    "with open('uni_and_bigram_tfidf_corpus.pkl', 'rb') as f:\n",
    "    uni_and_bigram_tfidf_corpus=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can try different regularization (l1, l2, or elastic net which is a combo of the two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuses = {'unigram_bow':unigram_bow_corpus, \n",
    "            'uni_bigram_bow': uni_and_bigram_bow_corpus, \n",
    "            'unigram_tfidf': unigram_tfidf_corpus, \n",
    "            'uni_bigram_tfidf': uni_and_bigram_tfidf_corpus}\n",
    "\n",
    "models = {'LogReg':LogisticRegression}\n",
    "\n",
    "def a_few_models(model_type = 'LogReg', corpus = 'unigram_bow', **kwargs):\n",
    "    print(\"--------------------------------------\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(corpuses[corpus], ratings, random_state=42)\n",
    "    \n",
    "    print(f\"MODEL TYPE: {model_type}; CORPUS: {corpus}\")\n",
    "    print(f\"MODEL PARAMETERS PROVIDED: {kwargs}\")\n",
    "    model = models[model_type](**kwargs)\n",
    "    print(model.get_params())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(f\"TRAINING ACCURACY: {accuracy_score(y_train, y_pred_train)}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"TEST DATA - CLASSIFICATION REPORT\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"WEIGHTED PRECISION: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"WEIGHTED Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"WEIGHTED F1: {f1_score(y_test, y_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 100}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.696688\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.82      0.77     17545\n",
      "         2.0       0.45      0.30      0.36     10235\n",
      "         3.0       0.47      0.36      0.41     13855\n",
      "         4.0       0.53      0.42      0.47     28358\n",
      "         5.0       0.74      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6379283452836969\n",
      "WEIGHTED Recall: 0.662776\n",
      "WEIGHTED F1: 0.6436408383729924\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 200}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 200, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.7063786666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.81      0.76     17545\n",
      "         2.0       0.44      0.32      0.37     10235\n",
      "         3.0       0.45      0.36      0.40     13855\n",
      "         4.0       0.53      0.42      0.47     28358\n",
      "         5.0       0.75      0.87      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6360036182877317\n",
      "WEIGHTED Recall: 0.65916\n",
      "WEIGHTED F1: 0.6423979020627302\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 300}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 300, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.708352\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.80      0.76     17545\n",
      "         2.0       0.43      0.32      0.37     10235\n",
      "         3.0       0.45      0.36      0.40     13855\n",
      "         4.0       0.53      0.42      0.47     28358\n",
      "         5.0       0.75      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6359817979932105\n",
      "WEIGHTED Recall: 0.659232\n",
      "WEIGHTED F1: 0.6423661776170081\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 100}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.748536\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.81      0.78     17545\n",
      "         2.0       0.46      0.36      0.40     10235\n",
      "         3.0       0.47      0.41      0.44     13855\n",
      "         4.0       0.53      0.46      0.49     28358\n",
      "         5.0       0.77      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.58      0.58    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6513387417009029\n",
      "WEIGHTED Recall: 0.668088\n",
      "WEIGHTED F1: 0.6571981829041221\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 200}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 200, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.7813173333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.79      0.77     17545\n",
      "         2.0       0.43      0.36      0.39     10235\n",
      "         3.0       0.45      0.39      0.42     13855\n",
      "         4.0       0.52      0.45      0.48     28358\n",
      "         5.0       0.76      0.84      0.80     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.57      0.57    125000\n",
      "weighted avg       0.64      0.66      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6421326487454257\n",
      "WEIGHTED Recall: 0.657592\n",
      "WEIGHTED F1: 0.6480072239106168\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 300}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 300, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.792152\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.78      0.76     17545\n",
      "         2.0       0.42      0.36      0.39     10235\n",
      "         3.0       0.44      0.40      0.42     13855\n",
      "         4.0       0.51      0.45      0.48     28358\n",
      "         5.0       0.76      0.84      0.80     55007\n",
      "\n",
      "    accuracy                           0.65    125000\n",
      "   macro avg       0.58      0.57      0.57    125000\n",
      "weighted avg       0.64      0.65      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6401111888527791\n",
      "WEIGHTED Recall: 0.654816\n",
      "WEIGHTED F1: 0.6457684357514549\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 100}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.689648\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.83      0.77     17545\n",
      "         2.0       0.46      0.30      0.36     10235\n",
      "         3.0       0.48      0.37      0.42     13855\n",
      "         4.0       0.53      0.48      0.50     28358\n",
      "         5.0       0.77      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.57      0.57    125000\n",
      "weighted avg       0.65      0.67      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6488112051757188\n",
      "WEIGHTED Recall: 0.669408\n",
      "WEIGHTED F1: 0.6548171565104824\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 200}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 200, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.698824\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.83      0.77     17545\n",
      "         2.0       0.47      0.31      0.37     10235\n",
      "         3.0       0.48      0.37      0.42     13855\n",
      "         4.0       0.53      0.47      0.50     28358\n",
      "         5.0       0.76      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.57      0.57    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6492729687903891\n",
      "WEIGHTED Recall: 0.66968\n",
      "WEIGHTED F1: 0.6551561240647015\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 300}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 300, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.7020373333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.83      0.78     17545\n",
      "         2.0       0.47      0.31      0.38     10235\n",
      "         3.0       0.47      0.37      0.42     13855\n",
      "         4.0       0.53      0.47      0.50     28358\n",
      "         5.0       0.76      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.57      0.58    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6495392229516812\n",
      "WEIGHTED Recall: 0.669464\n",
      "WEIGHTED F1: 0.6555383124603569\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 100}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.7173653333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.83      0.78     17545\n",
      "         2.0       0.49      0.33      0.39     10235\n",
      "         3.0       0.50      0.38      0.43     13855\n",
      "         4.0       0.54      0.50      0.52     28358\n",
      "         5.0       0.77      0.86      0.82     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.58      0.59    125000\n",
      "weighted avg       0.66      0.68      0.67    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6611203761166411\n",
      "WEIGHTED Recall: 0.678968\n",
      "WEIGHTED F1: 0.6661083943675812\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 200}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 200, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.74024\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.84      0.79     17545\n",
      "         2.0       0.49      0.33      0.40     10235\n",
      "         3.0       0.50      0.40      0.44     13855\n",
      "         4.0       0.54      0.49      0.52     28358\n",
      "         5.0       0.77      0.87      0.82     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.58      0.59    125000\n",
      "weighted avg       0.66      0.68      0.67    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6632726939311198\n",
      "WEIGHTED Recall: 0.681488\n",
      "WEIGHTED F1: 0.6685091975065341\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'n_jobs': 8, 'max_iter': 300}\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 300, 'multi_class': 'auto', 'n_jobs': 8, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "TRAINING ACCURACY: 0.7467066666666666\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.84      0.79     17545\n",
      "         2.0       0.49      0.34      0.40     10235\n",
      "         3.0       0.50      0.40      0.45     13855\n",
      "         4.0       0.54      0.49      0.52     28358\n",
      "         5.0       0.77      0.86      0.82     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.59      0.59    125000\n",
      "weighted avg       0.66      0.68      0.67    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6633612069886239\n",
      "WEIGHTED Recall: 0.681024\n",
      "WEIGHTED F1: 0.6686140968382178\n"
     ]
    }
   ],
   "source": [
    "for corpus_ in corpuses:\n",
    "    for n_iter in range(100, 301, 100):\n",
    "        a_few_models(corpus=corpus_ , n_jobs=8, max_iter=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization with liblinear solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.6922826666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.82      0.77     17545\n",
      "         2.0       0.47      0.24      0.32     10235\n",
      "         3.0       0.49      0.31      0.38     13855\n",
      "         4.0       0.52      0.40      0.45     28358\n",
      "         5.0       0.72      0.90      0.80     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.53      0.54    125000\n",
      "weighted avg       0.63      0.66      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6279951332173114\n",
      "WEIGHTED Recall: 0.65776\n",
      "WEIGHTED F1: 0.6308786788598448\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.6959413333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.82      0.77     17545\n",
      "         2.0       0.46      0.25      0.32     10235\n",
      "         3.0       0.47      0.30      0.37     13855\n",
      "         4.0       0.52      0.40      0.45     28358\n",
      "         5.0       0.72      0.90      0.80     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.53      0.54    125000\n",
      "weighted avg       0.63      0.66      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6269111769846345\n",
      "WEIGHTED Recall: 0.656608\n",
      "WEIGHTED F1: 0.6307989892489556\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.6973093333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.81      0.76     17545\n",
      "         2.0       0.45      0.25      0.32     10235\n",
      "         3.0       0.47      0.30      0.37     13855\n",
      "         4.0       0.51      0.40      0.45     28358\n",
      "         5.0       0.72      0.90      0.80     55007\n",
      "\n",
      "    accuracy                           0.65    125000\n",
      "   macro avg       0.58      0.53      0.54    125000\n",
      "weighted avg       0.62      0.65      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.624281570333396\n",
      "WEIGHTED Recall: 0.654064\n",
      "WEIGHTED F1: 0.6289171187202917\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7533173333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.82      0.78     17545\n",
      "         2.0       0.48      0.29      0.36     10235\n",
      "         3.0       0.49      0.35      0.41     13855\n",
      "         4.0       0.53      0.44      0.48     28358\n",
      "         5.0       0.74      0.89      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.60      0.56      0.57    125000\n",
      "weighted avg       0.64      0.67      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6442516749869124\n",
      "WEIGHTED Recall: 0.668792\n",
      "WEIGHTED F1: 0.6493380155066567\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7705973333333334\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.80      0.77     17545\n",
      "         2.0       0.44      0.30      0.36     10235\n",
      "         3.0       0.46      0.35      0.40     13855\n",
      "         4.0       0.52      0.44      0.47     28358\n",
      "         5.0       0.75      0.87      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.55      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6370955130076299\n",
      "WEIGHTED Recall: 0.660456\n",
      "WEIGHTED F1: 0.6436454264104199\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7804026666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.78      0.76     17545\n",
      "         2.0       0.42      0.31      0.35     10235\n",
      "         3.0       0.44      0.35      0.39     13855\n",
      "         4.0       0.51      0.43      0.47     28358\n",
      "         5.0       0.75      0.86      0.80     55007\n",
      "\n",
      "    accuracy                           0.65    125000\n",
      "   macro avg       0.57      0.55      0.55    125000\n",
      "weighted avg       0.63      0.65      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6300661879737428\n",
      "WEIGHTED Recall: 0.652064\n",
      "WEIGHTED F1: 0.6371189188154532\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.6777813333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.84      0.77     17545\n",
      "         2.0       0.50      0.22      0.31     10235\n",
      "         3.0       0.49      0.30      0.37     13855\n",
      "         4.0       0.52      0.46      0.48     28358\n",
      "         5.0       0.74      0.89      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.59      0.54      0.55    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6379773931347583\n",
      "WEIGHTED Recall: 0.664848\n",
      "WEIGHTED F1: 0.6403979407985202\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.688744\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.84      0.78     17545\n",
      "         2.0       0.49      0.24      0.32     10235\n",
      "         3.0       0.49      0.31      0.38     13855\n",
      "         4.0       0.52      0.46      0.49     28358\n",
      "         5.0       0.74      0.89      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.55      0.55    125000\n",
      "weighted avg       0.64      0.67      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6399249767584952\n",
      "WEIGHTED Recall: 0.666032\n",
      "WEIGHTED F1: 0.6436159065133175\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.697104\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.84      0.78     17545\n",
      "         2.0       0.48      0.25      0.33     10235\n",
      "         3.0       0.48      0.32      0.38     13855\n",
      "         4.0       0.52      0.46      0.49     28358\n",
      "         5.0       0.75      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.55      0.56    125000\n",
      "weighted avg       0.64      0.67      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6393467975297197\n",
      "WEIGHTED Recall: 0.665024\n",
      "WEIGHTED F1: 0.6442216073799001\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.691296\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.85      0.78     17545\n",
      "         2.0       0.53      0.25      0.34     10235\n",
      "         3.0       0.52      0.34      0.41     13855\n",
      "         4.0       0.53      0.47      0.50     28358\n",
      "         5.0       0.75      0.89      0.81     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.56      0.57    125000\n",
      "weighted avg       0.65      0.68      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6526115537650012\n",
      "WEIGHTED Recall: 0.676352\n",
      "WEIGHTED F1: 0.6539625081373316\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7154133333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.85      0.79     17545\n",
      "         2.0       0.52      0.28      0.36     10235\n",
      "         3.0       0.52      0.36      0.42     13855\n",
      "         4.0       0.54      0.48      0.51     28358\n",
      "         5.0       0.76      0.89      0.82     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.57      0.58    125000\n",
      "weighted avg       0.66      0.68      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6580189884583868\n",
      "WEIGHTED Recall: 0.680224\n",
      "WEIGHTED F1: 0.6610590458278442\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'solver': 'liblinear', 'penalty': 'l1', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7441333333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.84      0.79     17545\n",
      "         2.0       0.49      0.29      0.37     10235\n",
      "         3.0       0.49      0.36      0.42     13855\n",
      "         4.0       0.53      0.48      0.50     28358\n",
      "         5.0       0.76      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.60      0.57      0.58    125000\n",
      "weighted avg       0.65      0.68      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6541059010981936\n",
      "WEIGHTED Recall: 0.675832\n",
      "WEIGHTED F1: 0.659259378576467\n"
     ]
    }
   ],
   "source": [
    "for corpus_ in corpuses:\n",
    "    for C_ in [.5,1,2]: # Strength of regularization\n",
    "#         for l1_ratio_ in [.2, .5, .8]: # .2 is more L2 reg. than L1; .8 is more L1 than L2\n",
    "        a_few_models(corpus=corpus_ , C=C_, solver='liblinear', penalty='l1', verbose=20) # l1_ratio=l1_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization with lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.6988026666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.82      0.77     17545\n",
      "         2.0       0.45      0.31      0.37     10235\n",
      "         3.0       0.47      0.35      0.40     13855\n",
      "         4.0       0.53      0.43      0.47     28358\n",
      "         5.0       0.75      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6386146352326914\n",
      "WEIGHTED Recall: 0.6626\n",
      "WEIGHTED F1: 0.6448180438829698\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.696688\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.82      0.77     17545\n",
      "         2.0       0.45      0.30      0.36     10235\n",
      "         3.0       0.47      0.36      0.41     13855\n",
      "         4.0       0.53      0.42      0.47     28358\n",
      "         5.0       0.74      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6379283452836969\n",
      "WEIGHTED Recall: 0.662776\n",
      "WEIGHTED F1: 0.6436408383729924\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.6972746666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.82      0.77     17545\n",
      "         2.0       0.46      0.30      0.36     10235\n",
      "         3.0       0.47      0.35      0.40     13855\n",
      "         4.0       0.53      0.42      0.47     28358\n",
      "         5.0       0.74      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.55      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.637158965348075\n",
      "WEIGHTED Recall: 0.662424\n",
      "WEIGHTED F1: 0.6428910171825342\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.7502346666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.81      0.78     17545\n",
      "         2.0       0.46      0.36      0.40     10235\n",
      "         3.0       0.47      0.41      0.44     13855\n",
      "         4.0       0.53      0.43      0.48     28358\n",
      "         5.0       0.76      0.87      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.58      0.58    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6493088953219472\n",
      "WEIGHTED Recall: 0.668304\n",
      "WEIGHTED F1: 0.655093180280955\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.7485333333333334\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.81      0.78     17545\n",
      "         2.0       0.46      0.36      0.40     10235\n",
      "         3.0       0.47      0.41      0.44     13855\n",
      "         4.0       0.53      0.46      0.49     28358\n",
      "         5.0       0.77      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.58      0.58    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6513387417009029\n",
      "WEIGHTED Recall: 0.668088\n",
      "WEIGHTED F1: 0.6571981829041221\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.749176\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.81      0.78     17545\n",
      "         2.0       0.46      0.36      0.40     10235\n",
      "         3.0       0.47      0.40      0.43     13855\n",
      "         4.0       0.53      0.45      0.49     28358\n",
      "         5.0       0.76      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.58      0.58    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6498413923195301\n",
      "WEIGHTED Recall: 0.667576\n",
      "WEIGHTED F1: 0.6558415137489275\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.6892026666666666\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.83      0.77     17545\n",
      "         2.0       0.47      0.28      0.35     10235\n",
      "         3.0       0.47      0.39      0.43     13855\n",
      "         4.0       0.53      0.47      0.50     28358\n",
      "         5.0       0.76      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.57      0.57    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6494102606545963\n",
      "WEIGHTED Recall: 0.669752\n",
      "WEIGHTED F1: 0.655083064257158\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.689648\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.83      0.77     17545\n",
      "         2.0       0.46      0.30      0.36     10235\n",
      "         3.0       0.48      0.37      0.42     13855\n",
      "         4.0       0.53      0.48      0.50     28358\n",
      "         5.0       0.77      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.57      0.57    125000\n",
      "weighted avg       0.65      0.67      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6488112051757188\n",
      "WEIGHTED Recall: 0.669408\n",
      "WEIGHTED F1: 0.6548171565104824\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.6897813333333334\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.82      0.77     17545\n",
      "         2.0       0.45      0.31      0.37     10235\n",
      "         3.0       0.47      0.38      0.42     13855\n",
      "         4.0       0.53      0.49      0.51     28358\n",
      "         5.0       0.77      0.85      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.57      0.58    125000\n",
      "weighted avg       0.65      0.67      0.66    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6499226305691314\n",
      "WEIGHTED Recall: 0.668104\n",
      "WEIGHTED F1: 0.6559383060354157\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.7105733333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.83      0.78     17545\n",
      "         2.0       0.50      0.32      0.39     10235\n",
      "         3.0       0.51      0.37      0.43     13855\n",
      "         4.0       0.54      0.49      0.52     28358\n",
      "         5.0       0.77      0.87      0.82     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.58      0.59    125000\n",
      "weighted avg       0.66      0.68      0.67    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6609777041589375\n",
      "WEIGHTED Recall: 0.6806\n",
      "WEIGHTED F1: 0.6656007449201992\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.7173653333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.83      0.78     17545\n",
      "         2.0       0.49      0.33      0.39     10235\n",
      "         3.0       0.50      0.38      0.43     13855\n",
      "         4.0       0.54      0.50      0.52     28358\n",
      "         5.0       0.77      0.86      0.82     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.61      0.58      0.59    125000\n",
      "weighted avg       0.66      0.68      0.67    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6611203761166411\n",
      "WEIGHTED Recall: 0.678968\n",
      "WEIGHTED F1: 0.6661083943675812\n",
      "--------------------------------------\n",
      "MODEL TYPE: LogReg; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'penalty': 'l2', 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 20, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.7189946666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.83      0.78     17545\n",
      "         2.0       0.48      0.34      0.40     10235\n",
      "         3.0       0.49      0.40      0.44     13855\n",
      "         4.0       0.54      0.49      0.51     28358\n",
      "         5.0       0.78      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.68    125000\n",
      "   macro avg       0.60      0.58      0.59    125000\n",
      "weighted avg       0.66      0.68      0.67    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6608930457345766\n",
      "WEIGHTED Recall: 0.6778\n",
      "WEIGHTED F1: 0.6663600226821427\n"
     ]
    }
   ],
   "source": [
    "for corpus_ in corpuses:\n",
    "    for C_ in [.5,1,2]: # Strength of regularization\n",
    "#         for l1_ratio_ in [.2, .5, .8]: # .2 is more L2 reg. than L1; .8 is more L1 than L2\n",
    "        a_few_models(corpus=corpus_ , C=C_, penalty='l2', verbose=20) # l1_ratio=l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ratings.pkl', 'wb') as f:\n",
    "    pickle.dump(ratings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'label']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [1, 5]\n",
    "keys = ['label']*len(pred)\n",
    "keys\n",
    "# dict(zip(keys,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuses = {'unigram_bow':unigram_bow_corpus, \n",
    "            'uni_bigram_bow': uni_and_bigram_bow_corpus, \n",
    "            'unigram_tfidf': unigram_tfidf_corpus, \n",
    "            'uni_bigram_tfidf': uni_and_bigram_tfidf_corpus}\n",
    "\n",
    "models = {'LogReg':LogisticRegression, 'SVM':LinearSVC}\n",
    "\n",
    "def a_few_models(model_type = 'LogReg', corpus = 'unigram_bow', **kwargs):\n",
    "    print(\"--------------------------------------\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(corpuses[corpus], ratings, random_state=42)\n",
    "    \n",
    "    print(f\"MODEL TYPE: {model_type}; CORPUS: {corpus}\")\n",
    "    print(f\"MODEL PARAMETERS PROVIDED: {kwargs}\")\n",
    "    model = models[model_type](**kwargs)\n",
    "    print(model.get_params())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(f\"TRAINING ACCURACY: {accuracy_score(y_train, y_pred_train)}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"TEST DATA - CLASSIFICATION REPORT\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"WEIGHTED PRECISION: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"WEIGHTED Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"WEIGHTED F1: {f1_score(y_test, y_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristiyan/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.691936\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.82      0.76     17545\n",
      "         2.0       0.45      0.24      0.31     10235\n",
      "         3.0       0.47      0.29      0.36     13855\n",
      "         4.0       0.52      0.38      0.44     28358\n",
      "         5.0       0.71      0.91      0.80     55007\n",
      "\n",
      "    accuracy                           0.65    125000\n",
      "   macro avg       0.57      0.53      0.53    125000\n",
      "weighted avg       0.62      0.65      0.62    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.618654166568615\n",
      "WEIGHTED Recall: 0.650976\n",
      "WEIGHTED F1: 0.6212519452949821\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.691816\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.82      0.76     17545\n",
      "         2.0       0.45      0.23      0.31     10235\n",
      "         3.0       0.47      0.28      0.35     13855\n",
      "         4.0       0.52      0.38      0.44     28358\n",
      "         5.0       0.71      0.91      0.80     55007\n",
      "\n",
      "    accuracy                           0.65    125000\n",
      "   macro avg       0.57      0.53      0.53    125000\n",
      "weighted avg       0.62      0.65      0.62    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6184828975465273\n",
      "WEIGHTED Recall: 0.651088\n",
      "WEIGHTED F1: 0.6212451189997001\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: unigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.693432\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.82      0.76     17545\n",
      "         2.0       0.44      0.24      0.31     10235\n",
      "         3.0       0.46      0.31      0.37     13855\n",
      "         4.0       0.52      0.39      0.44     28358\n",
      "         5.0       0.72      0.90      0.80     55007\n",
      "\n",
      "    accuracy                           0.65    125000\n",
      "   macro avg       0.57      0.53      0.54    125000\n",
      "weighted avg       0.62      0.65      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6208603475267326\n",
      "WEIGHTED Recall: 0.652024\n",
      "WEIGHTED F1: 0.6252335835794839\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.780888\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.77      0.74     17545\n",
      "         2.0       0.39      0.31      0.35     10235\n",
      "         3.0       0.43      0.34      0.38     13855\n",
      "         4.0       0.51      0.41      0.46     28358\n",
      "         5.0       0.74      0.86      0.80     55007\n",
      "\n",
      "    accuracy                           0.64    125000\n",
      "   macro avg       0.56      0.54      0.54    125000\n",
      "weighted avg       0.62      0.64      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6220350645522692\n",
      "WEIGHTED Recall: 0.644776\n",
      "WEIGHTED F1: 0.6291949748461919\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7824266666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.76      0.73     17545\n",
      "         2.0       0.39      0.31      0.34     10235\n",
      "         3.0       0.43      0.34      0.38     13855\n",
      "         4.0       0.51      0.41      0.45     28358\n",
      "         5.0       0.74      0.86      0.80     55007\n",
      "\n",
      "    accuracy                           0.64    125000\n",
      "   macro avg       0.55      0.54      0.54    125000\n",
      "weighted avg       0.62      0.64      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6202175759631077\n",
      "WEIGHTED Recall: 0.642696\n",
      "WEIGHTED F1: 0.627399836121394\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: uni_bigram_bow\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7821546666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.75      0.73     17545\n",
      "         2.0       0.38      0.30      0.34     10235\n",
      "         3.0       0.42      0.34      0.38     13855\n",
      "         4.0       0.50      0.41      0.45     28358\n",
      "         5.0       0.74      0.86      0.80     55007\n",
      "\n",
      "    accuracy                           0.64    125000\n",
      "   macro avg       0.55      0.53      0.54    125000\n",
      "weighted avg       0.62      0.64      0.63    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6183206417458575\n",
      "WEIGHTED Recall: 0.64076\n",
      "WEIGHTED F1: 0.6256122183018166\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.6961146666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.85      0.77     17545\n",
      "         2.0       0.47      0.23      0.31     10235\n",
      "         3.0       0.48      0.29      0.36     13855\n",
      "         4.0       0.52      0.44      0.47     28358\n",
      "         5.0       0.74      0.89      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.54      0.54    125000\n",
      "weighted avg       0.63      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6320546238580363\n",
      "WEIGHTED Recall: 0.660832\n",
      "WEIGHTED F1: 0.6361916448295543\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.69784\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.84      0.77     17545\n",
      "         2.0       0.46      0.24      0.31     10235\n",
      "         3.0       0.47      0.29      0.36     13855\n",
      "         4.0       0.51      0.44      0.47     28358\n",
      "         5.0       0.74      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.54      0.54    125000\n",
      "weighted avg       0.63      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6309304419180451\n",
      "WEIGHTED Recall: 0.659608\n",
      "WEIGHTED F1: 0.6357501195654304\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: unigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.6989466666666667\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.84      0.77     17545\n",
      "         2.0       0.45      0.24      0.32     10235\n",
      "         3.0       0.47      0.30      0.36     13855\n",
      "         4.0       0.51      0.44      0.47     28358\n",
      "         5.0       0.74      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.54      0.54    125000\n",
      "weighted avg       0.63      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6300870952620471\n",
      "WEIGHTED Recall: 0.658672\n",
      "WEIGHTED F1: 0.635371169483572\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 0.5, 'verbose': 20}\n",
      "{'C': 0.5, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.758584\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.84      0.78     17545\n",
      "         2.0       0.47      0.29      0.36     10235\n",
      "         3.0       0.48      0.35      0.40     13855\n",
      "         4.0       0.52      0.46      0.49     28358\n",
      "         5.0       0.76      0.87      0.81     55007\n",
      "\n",
      "    accuracy                           0.67    125000\n",
      "   macro avg       0.59      0.56      0.57    125000\n",
      "weighted avg       0.65      0.67      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6451269308108037\n",
      "WEIGHTED Recall: 0.668696\n",
      "WEIGHTED F1: 0.6508987192368484\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 1, 'verbose': 20}\n",
      "{'C': 1, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7675573333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.83      0.77     17545\n",
      "         2.0       0.45      0.30      0.36     10235\n",
      "         3.0       0.47      0.35      0.40     13855\n",
      "         4.0       0.52      0.45      0.48     28358\n",
      "         5.0       0.76      0.86      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.65    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6407639427232423\n",
      "WEIGHTED Recall: 0.663072\n",
      "WEIGHTED F1: 0.6473509932514633\n",
      "--------------------------------------\n",
      "MODEL TYPE: SVM; CORPUS: uni_bigram_tfidf\n",
      "MODEL PARAMETERS PROVIDED: {'C': 2, 'verbose': 20}\n",
      "{'C': 2, 'class_weight': None, 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 1000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 20}\n",
      "[LibLinear]TRAINING ACCURACY: 0.7737653333333333\n",
      "TEST DATA - CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.81      0.77     17545\n",
      "         2.0       0.43      0.31      0.36     10235\n",
      "         3.0       0.45      0.35      0.40     13855\n",
      "         4.0       0.51      0.45      0.48     28358\n",
      "         5.0       0.76      0.86      0.80     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n",
      "WEIGHTED PRECISION: 0.6367229158834767\n",
      "WEIGHTED Recall: 0.657904\n",
      "WEIGHTED F1: 0.6436552035523088\n"
     ]
    }
   ],
   "source": [
    "for corpus_ in corpuses:\n",
    "    for C_ in [.5,1,2]: # Strength of regularization\n",
    "        a_few_models(model_type = 'SVM', corpus=corpus_ , C=C_, verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3aaf935e6aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load the sparse matrix file that’s being written here, you can use this: from sklearn.datasets import load_svmlight_file ",
    "X,y = load_svmlight_file('./review.liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with gensim - the long way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to create a separate set of reviews, which contains unigrams & bigrams\n",
    "def uni_and_bigrams(tokens):\n",
    "    # the bigrams just contatenate 2 adjacent tokens with _ in between\n",
    "    bigrams=list(map(lambda x: '_'.join(x), zip(tokens, tokens[1:])))\n",
    "    # returning a list containing all 1 and 2-grams\n",
    "    return tokens+bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool=multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "preprocessed_reviews_with_bigrams = pool.map(uni_and_bigrams, preprocessed_reviews)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving preprocessed reviews with bigrams to pickle file\n",
    "# with open('preprocessed_reviews_with_bigrams.pkl', 'wb') as f:\n",
    "#     pickle.dump(preprocessed_reviews_with_bigrams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading in the pickled preprocessed reviews with bigrams\n",
    "# with open('preprocessed_reviews_with_bigrams.pkl', 'rb') as f:\n",
    "#     preprocessed_reviews_with_bigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for unigrams\n",
    "unigram_dict = gensim.corpora.Dictionary(preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total vocabulary size is 158k unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to convert the two corpuses to BoW representation\n",
    "unigram_dict.filter_extremes(no_below = 100, no_above = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11147"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size is drastically reduced to just 11,147 words after removing rare words appearing in fewer than 100 review and very frequent ones in more than half of all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to pickle file\n",
    "unigram_dict.save('unigram_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for unigrams & bigrams\n",
    "uni_and_bigram_dict = gensim.corpora.Dictionary(preprocessed_reviews_with_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2176877"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_and_bigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total vocabulary size is now ~ 10x larger at 2.2 million unigrams and bigrams than the original unigram vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_and_bigram_dict.filter_extremes(no_below = 100, no_above = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34936"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uni_and_bigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary is again drastically reduced to just ~35k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to pickle file\n",
    "uni_and_bigram_dict.save('uni_and_bigram_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to BOW and TFIDF w/ gensim -> works, but breaks when I try to use it to fit LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to have both a BOW and TFIDF representation so I can compare performance in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW corpus for unigrams\n",
    "unigram_bow_corpus = [unigram_dict.doc2bow(review) for review in preprocessed_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW corpus for unigrams & bigrams\n",
    "uni_and_bigram_bow_corpus = [uni_and_bigram_dict.doc2bow(review) for review in preprocessed_reviews_with_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF corpus for unigrams \n",
    "unigram_tfidf_model = gensim.models.TfidfModel(corpus=unigram_bow_corpus)\n",
    "unigram_tfidf_corpus = unigram_tfidf_model[unigram_bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF corpus for unigrams & bigrams\n",
    "unigram_tfidf_model = gensim.models.TfidfModel(dictionary=unigram_dict)\n",
    "unigram_tfidf_corpus = unigram_tfidf_model[unigram_bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg - the long way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(unigram_bow_corpus, ratings, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogReg with defaults is: penalty = l2, tol (tolerance) is 10^-4, C (inverse of regularization strength) = 1 (smaller number = stronger regularization)\n",
    "# max_iter = 100 (maximum solver iterations), solver = 'lbfgs'\n",
    "model = LogisticRegression(n_jobs=8)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.696688"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Accuracy\n",
    "y_pred_train = model.predict(X_train)\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14347,  1565,   530,   295,   808],\n",
       "       [ 3526,  3084,  2183,   795,   647],\n",
       "       [ 1168,  1653,  5024,  4000,  2010],\n",
       "       [  404,   348,  2480, 11793, 13333],\n",
       "       [  459,   143,   542,  5264, 48599]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.82      0.77     17545\n",
      "         2.0       0.45      0.30      0.36     10235\n",
      "         3.0       0.47      0.36      0.41     13855\n",
      "         4.0       0.53      0.42      0.47     28358\n",
      "         5.0       0.74      0.88      0.81     55007\n",
      "\n",
      "    accuracy                           0.66    125000\n",
      "   macro avg       0.58      0.56      0.56    125000\n",
      "weighted avg       0.64      0.66      0.64    125000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.662776"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6379283452836969"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average='weighted') # verifying calculation: (.72*17545 + .45*10235 + .47*13855 + .53*28358 + .74*55007)/125000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.662776"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6436408383729924"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 200 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, n_jobs=8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogReg with defaults is: penalty = l2, tol (tolerance) is 10^-4, C (inverse of regularization strength) = 1 (smaller number = stronger regularization)\n",
    "# max_iter = 100 (maximum solver iterations), solver = 'lbfgs'\n",
    "model = LogisticRegression(n_jobs=8, max_iter = 200)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7063786666666667"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Accuracy\n",
    "y_pred_train = model.predict(X_train)\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65916"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6360036182877317"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65916"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6423979020627302"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 500 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=500, n_jobs=8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogReg with defaults is: penalty = l2, tol (tolerance) is 10^-4, C (inverse of regularization strength) = 1 (smaller number = stronger regularization)\n",
    "# max_iter = 100 (maximum solver iterations), solver = 'lbfgs'\n",
    "model = LogisticRegression(n_jobs=8, max_iter = 500)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7094293333333334"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Accuracy\n",
    "y_pred_train = model.predict(X_train)\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658264"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6350648974804008"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658264"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6415278269031528"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, n_jobs=8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogReg with defaults is: penalty = l2, tol (tolerance) is 10^-4, C (inverse of regularization strength) = 1 (smaller number = stronger regularization)\n",
    "# max_iter = 100 (maximum solver iterations), solver = 'lbfgs'\n",
    "model = LogisticRegression(n_jobs=8, max_iter = 1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7096613333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Accuracy\n",
    "y_pred_train = model.predict(X_train)\n",
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6353323647024285"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658512"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6417969581340978"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try CV with LogReg\n",
    "Takes ages, cancelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegressionCV(Cs = 3, cv=3, random_state=42, n_jobs=5).fit(unigram_bow_corpus, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing with parallelism & tqdm?\n",
    "Getting unpleasant macOS error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049c85eb4c644ccc82de79b00d92a2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "completed batch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fb341347bf4529a47aa9d145b8c0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "completed batch 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a755983e56c4762aefe26f828188f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-930368143572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpreprocessed_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"completed batch {batch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/p_tqdm/p_tqdm.py\u001b[0m in \u001b[0;36mp_map\u001b[0;34m(function, *iterables, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/p_tqdm/p_tqdm.py\u001b[0m in \u001b[0;36m_parallel\u001b[0;34m(ordered, function, *iterables, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmap_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Below uses p_map which is a wrapper for tqdm and multiprocessing.\n",
    "# Due to some Python/macOS Catalina issues, this doesn't appear to be working\n",
    "# See: https://github.com/gammapy/gammapy/issues/2453\n",
    "\n",
    "preprocessed_reviews = []\n",
    "\n",
    "for batch in batches:\n",
    "    \n",
    "    batch_reviews = reviews[batch:batch+10_000]\n",
    "    \n",
    "    preprocessed_batch = p_map(preprocess_text, reviews[:100])\n",
    "    \n",
    "    print(f\"completed batch {batch}\")\n",
    "    \n",
    "    preprocessed_reviews.extend(preprocessed_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
